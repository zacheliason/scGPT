{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Running on 2025-02-06 21:28:37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n",
      "Local copy of pyg dataset is detected. Loading...\n",
      "Done!\n",
      "Local copy of split is detected. Loading...\n",
      "Simulation split test composition:\n",
      "combo_seen0:0\n",
      "combo_seen1:0\n",
      "combo_seen2:0\n",
      "unseen_single:22\n",
      "Done!\n",
      "Creating dataloaders....\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Memory usage after clearing:\n"
     ]
    }
   ],
   "source": [
    "# import argparse\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import warnings\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import psutil\n",
    "import scgpt as scg\n",
    "import torch\n",
    "from gears import PertData\n",
    "from gears.utils import create_cell_graph_dataset_for_prediction\n",
    "from scgpt.model import TransformerGenerator\n",
    "from scgpt.tokenizer.gene_tokenizer import GeneVocab\n",
    "from scgpt.utils import map_raw_id_to_vocab_id, set_seed\n",
    "from torch import nn\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torchtext._torchtext import (\n",
    "    Vocab as VocabPybind,\n",
    ")\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "\n",
    "def prepare_data(\n",
    "    batch_size,\n",
    "    test_batch_size,\n",
    "    data_dir,\n",
    "    data_name=\"adamson\",\n",
    "    split=\"simulation\",\n",
    "):\n",
    "    pert_data = PertData(data_dir)\n",
    "    pert_data.load(data_name=data_name)\n",
    "    pert_data.prepare_split(split=split, seed=1)\n",
    "    pert_data.get_dataloader(batch_size=batch_size, test_batch_size=test_batch_size)\n",
    "    pert_data = clear_pert_data_memory(pert_data)\n",
    "\n",
    "    genes = pert_data.adata.var[\"gene_name\"].tolist()\n",
    "    n_genes = len(genes)\n",
    "    vocab = Vocab(\n",
    "        VocabPybind(genes + special_tokens, None)\n",
    "    )  # bidirectional lookup [gene <-> int]\n",
    "    vocab.set_default_index(vocab[\"<pad>\"])\n",
    "\n",
    "    gene_ids = np.array(\n",
    "        [vocab[gene] if gene in vocab else vocab[\"<pad>\"] for gene in genes], dtype=int\n",
    "    )\n",
    "\n",
    "    return pert_data, vocab, gene_ids, n_genes\n",
    "\n",
    "\n",
    "def setup_model(load_model, vocab, genes, default_config):\n",
    "    # Unpack default model settings\n",
    "    embsize = default_config[\"embsize\"]\n",
    "    d_hid = default_config[\"d_hid\"]\n",
    "    nlayers = default_config[\"nlayers\"]\n",
    "    nhead = default_config[\"nhead\"]\n",
    "    n_layers_cls = default_config[\"n_layers_cls\"]\n",
    "    dropout = default_config[\"dropout\"]\n",
    "    use_fast_transformer = default_config[\"use_fast_transformer\"]\n",
    "    pretrained_dict = None\n",
    "\n",
    "    # Update model settings from checkpoint, if provided\n",
    "    if load_model is not None and os.path.exists(load_model):\n",
    "        model_dir = os.path.join(load_model)\n",
    "        model_config_file = os.path.join(model_dir, \"args.json\")\n",
    "        model_file = os.path.join(model_dir, \"best_model.pt\")\n",
    "        vocab_file = os.path.join(model_dir, \"vocab.json\")\n",
    "\n",
    "        # Update vocab if loading from existing model\n",
    "        loaded_vocab = GeneVocab.from_file(vocab_file)\n",
    "        for s in special_tokens:\n",
    "            if s not in loaded_vocab:\n",
    "                loaded_vocab.append_token(s)\n",
    "\n",
    "        # Check gene vocabulary matching\n",
    "        gene_ids_in_vocab = np.array(\n",
    "            [1 if gene in loaded_vocab else -1 for gene in genes]\n",
    "        )\n",
    "        logger.info(\n",
    "            f\"match {np.sum(gene_ids_in_vocab >= 0)}/{len(gene_ids_in_vocab)} genes \"\n",
    "            f\"in vocabulary of size {len(loaded_vocab)}.\"\n",
    "        )\n",
    "\n",
    "        # Load model configurations\n",
    "        with open(model_config_file, \"r\") as f:\n",
    "            model_configs = json.load(f)\n",
    "\n",
    "        logger.info(\n",
    "            f\"Resume model from {model_file}, the model args will override the \"\n",
    "            f\"config {model_config_file}.\"\n",
    "        )\n",
    "\n",
    "        # Extract model parameters\n",
    "        embsize = model_configs[\"embsize\"]\n",
    "        nhead = model_configs[\"nheads\"]\n",
    "        d_hid = model_configs[\"d_hid\"]\n",
    "        nlayers = model_configs[\"nlayers\"]\n",
    "        n_layers_cls = model_configs[\"n_layers_cls\"]\n",
    "\n",
    "        # Load pretrained weights\n",
    "        pretrained_dict = torch.load(model_file, map_location=torch.device(device))\n",
    "        pretrained_dict = convert_wqkv_to_in_proj(pretrained_dict)\n",
    "    else:\n",
    "        genes = pert_data.adata.var[\"gene_name\"].tolist()\n",
    "        vocab = Vocab(\n",
    "            VocabPybind(genes + special_tokens, None)\n",
    "        )  # bidirectional lookup [gene <-> int]\n",
    "\n",
    "    ntokens = len(vocab)  # size of vocabulary\n",
    "    model = TransformerGenerator(\n",
    "        ntokens,\n",
    "        embsize,\n",
    "        nhead,\n",
    "        d_hid,\n",
    "        nlayers,\n",
    "        nlayers_cls=n_layers_cls,\n",
    "        n_cls=1,\n",
    "        vocab=vocab,\n",
    "        dropout=dropout,\n",
    "        pad_token=pad_token,\n",
    "        pad_value=pad_value,\n",
    "        pert_pad_id=pert_pad_id,\n",
    "        use_fast_transformer=use_fast_transformer,\n",
    "    )\n",
    "\n",
    "    # Load pretrained weights\n",
    "    if (\n",
    "        load_param_prefixs is not None\n",
    "        and load_model is not None\n",
    "        and pretrained_dict is not None\n",
    "    ):\n",
    "        # only load params that start with the prefix\n",
    "        model_dict = model.state_dict()\n",
    "        pretrained_dict = {\n",
    "            k: v\n",
    "            for k, v in pretrained_dict.items()\n",
    "            if any([k.startswith(prefix) for prefix in load_param_prefixs])\n",
    "        }\n",
    "        for k, v in pretrained_dict.items():\n",
    "            logger.info(f\"Loading params {k} with shape {v.shape}\")\n",
    "        model_dict.update(pretrained_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "    elif load_model is not None:\n",
    "        try:\n",
    "            model.load_state_dict(torch.load(model_file))\n",
    "            logger.info(f\"Loading all model params from {model_file}\")\n",
    "        except RuntimeError:\n",
    "            # only load params that are in the model and match the size\n",
    "            model_dict = model.state_dict()\n",
    "            pretrained_dict = torch.load(model_file)\n",
    "            pretrained_dict = {\n",
    "                k: v\n",
    "                for k, v in pretrained_dict.items()\n",
    "                if k in model_dict and v.shape == model_dict[k].shape\n",
    "            }\n",
    "            for k, v in pretrained_dict.items():\n",
    "                logger.info(f\"Loading params {k} with shape {v.shape}\")\n",
    "            model_dict.update(pretrained_dict)\n",
    "            model.load_state_dict(model_dict)\n",
    "\n",
    "    model.to(device)\n",
    "    enable_gradient_checkpointing(model)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def print_memory_usage():\n",
    "    # RAM usage\n",
    "    process = psutil.Process()\n",
    "    ram_usage = process.memory_info().rss / 1024 / 1024  # in MB\n",
    "\n",
    "    # GPU memory if available\n",
    "    gpu_usage = None\n",
    "    # TODO return from metal\n",
    "    # if torch.cuda.is_available():\n",
    "    #     gpu_usage = torch.cuda.memory_allocated() / 1024 / 1024  # in MB\n",
    "\n",
    "    # print(f\"RAM Usage: {ram_usage:.2f} MB\")\n",
    "    # if gpu_usage is not None:\n",
    "    #     print(f\"GPU Memory Usage: {gpu_usage:.2f} MB\")\n",
    "\n",
    "\n",
    "def clear_pert_data_memory(pert_data):\n",
    "    # List of attributes that can be safely cleared\n",
    "    attributes_to_clear = [\n",
    "        \"processed_data\",\n",
    "        \"raw_data\",\n",
    "        \"train_control_data\",\n",
    "        \"val_control_data\",\n",
    "        \"test_control_data\",\n",
    "    ]\n",
    "\n",
    "    # Clear attributes\n",
    "    for attr in attributes_to_clear:\n",
    "        if hasattr(pert_data, attr):\n",
    "            delattr(pert_data, attr)\n",
    "\n",
    "    # Force garbage collection\n",
    "    gc.collect()\n",
    "\n",
    "    # TODO return from metal\n",
    "    # if torch.cuda.is_available():\n",
    "    #     torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"\\nMemory usage after clearing:\")\n",
    "    print_memory_usage()\n",
    "\n",
    "    return pert_data\n",
    "\n",
    "\n",
    "def convert_wqkv_to_in_proj(pretrained_state_dict):\n",
    "    converted_state_dict = {}\n",
    "\n",
    "    # for key, value in pretrained_state_dict.items():\n",
    "    for key, value in pretrained_state_dict.items():\n",
    "        # if \"self_attn.Wqkv.weight\" in key:\n",
    "        # print(key)\n",
    "        if \"self_attn.in_proj_weight\" in key or \"self_attn.Wqkv.weight\" in key:\n",
    "            # print(f\"{value.shape}\\t{key}\")\n",
    "            converted_state_dict[\n",
    "                key.replace(\"self_attn.Wqkv.weight\", \"self_attn.in_proj_weight\")\n",
    "            ] = value\n",
    "        elif \"self_attn.in_proj_bias\" in key or \"self_attn.Wqkv.bias\" in key:\n",
    "            # print(f\"{value.shape}\\t{key}\")\n",
    "            converted_state_dict[\n",
    "                key.replace(\"self_attn.Wqkv.bias\", \"self_attn.in_proj_bias\")\n",
    "            ] = value\n",
    "        else:\n",
    "            converted_state_dict[key] = value\n",
    "\n",
    "    return converted_state_dict\n",
    "\n",
    "\n",
    "def train(\n",
    "    model: nn.Module,\n",
    "    train_loader: torch.utils.data.DataLoader,\n",
    "    n_genes: int,\n",
    "    criterion: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scheduler: torch.optim.lr_scheduler._LRScheduler,\n",
    "    scaler: any,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Train the model for one epoch with gradient accumulation.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss, total_mse = 0.0, 0.0\n",
    "    start_time = time.time()\n",
    "\n",
    "    num_batches = len(train_loader)\n",
    "    optimizer.zero_grad()  # Zero gradients at the start\n",
    "\n",
    "    for batch, batch_data in enumerate(train_loader):\n",
    "        actual_batch_size = len(batch_data.y)\n",
    "        batch_data.to(device)\n",
    "        x: torch.Tensor = batch_data.x  # (batch_size * n_genes, 2)\n",
    "\n",
    "        print(f\"Shape of x: {x.shape}\")\n",
    "        print(f\"Values of x:\\n{x}\")\n",
    "\n",
    "        ori_gene_values = x[:, 0].view(actual_batch_size, n_genes)\n",
    "        pert_flags = x[:, 1].long().view(actual_batch_size, n_genes)\n",
    "\n",
    "        print(f\"Original gene values shape: {ori_gene_values.shape}\")\n",
    "        print(f\"Original gene values:\\n{ori_gene_values}\")\n",
    "        print(f\"Perturbation flags shape: {pert_flags.shape}\")\n",
    "        print(f\"Perturbation flags:\\n{pert_flags}\")\n",
    "\n",
    "        target_gene_values = batch_data.y  # (batch_size, n_genes)\n",
    "\n",
    "        if include_zero_gene in [\"all\", \"batch-wise\"]:\n",
    "            if include_zero_gene == \"all\":\n",
    "                input_gene_ids = torch.arange(n_genes, device=device, dtype=torch.long)\n",
    "            else:\n",
    "                input_gene_ids = (\n",
    "                    ori_gene_values.nonzero()[:, 1].flatten().unique().sort()[0]\n",
    "                )\n",
    "            # sample input_gene_id\n",
    "            if len(input_gene_ids) > max_seq_len:\n",
    "                input_gene_ids = torch.randperm(len(input_gene_ids), device=device)[\n",
    "                    :max_seq_len\n",
    "                ]\n",
    "            input_values = ori_gene_values[:, input_gene_ids]\n",
    "            input_pert_flags = pert_flags[:, input_gene_ids]\n",
    "            target_values = target_gene_values[:, input_gene_ids]\n",
    "\n",
    "            mapped_input_gene_ids = map_raw_id_to_vocab_id(input_gene_ids, gene_ids)\n",
    "            mapped_input_gene_ids = mapped_input_gene_ids.repeat(actual_batch_size, 1)\n",
    "\n",
    "            src_key_padding_mask = torch.zeros_like(\n",
    "                input_values, dtype=torch.bool, device=device\n",
    "            )\n",
    "\n",
    "        # TODO return from metal\n",
    "        # with torch.cuda.amp.autocast(enabled=amp):\n",
    "        with torch.amp.autocast(enabled=amp):\n",
    "            output_dict = model(\n",
    "                mapped_input_gene_ids,\n",
    "                input_values,\n",
    "                input_pert_flags,\n",
    "                src_key_padding_mask=src_key_padding_mask,\n",
    "                CLS=CLS,\n",
    "                CCE=CCE,\n",
    "                MVC=MVC,\n",
    "                ECS=ECS,\n",
    "            )\n",
    "            output_values = output_dict[\"mlm_output\"]\n",
    "\n",
    "            masked_positions = torch.ones_like(\n",
    "                input_values, dtype=torch.bool\n",
    "            )  # Use all\n",
    "            loss = loss_mse = criterion(output_values, target_values, masked_positions)\n",
    "\n",
    "            # Normalize loss by accumulation steps\n",
    "            loss = loss / accumulation_steps\n",
    "\n",
    "        # Accumulate gradients\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Update weights only after accumulating gradients for specified steps\n",
    "        if (batch + 1) % accumulation_steps == 0 or (batch + 1) == num_batches:\n",
    "            scaler.unscale_(optimizer)\n",
    "            with warnings.catch_warnings(record=True) as w:\n",
    "                warnings.filterwarnings(\"always\")\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    model.parameters(),\n",
    "                    1.0,\n",
    "                    error_if_nonfinite=False if scaler.is_enabled() else True,\n",
    "                )\n",
    "                if len(w) > 0:\n",
    "                    logger.warning(\n",
    "                        f\"Found infinite gradient. This may be caused by the gradient \"\n",
    "                        f\"scaler. The current scale is {scaler.get_scale()}. This warning \"\n",
    "                        \"can be ignored if no longer occurs after autoscaling of the scaler.\"\n",
    "                    )\n",
    "\n",
    "            # Step optimizer and scaler\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()  # Reset gradients\n",
    "\n",
    "        # Multiply loss by accumulation_steps to get the equivalent loss for logging\n",
    "        total_loss += loss.item() * accumulation_steps\n",
    "        total_mse += loss_mse.item() * accumulation_steps\n",
    "\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            cur_mse = total_mse / log_interval\n",
    "            logger.info(\n",
    "                f\"| epoch {epoch:3d} | {batch:3d}/{num_batches:3d} batches | \"\n",
    "                f\"lr {lr:05.4f} | ms/batch {ms_per_batch:5.2f} | \"\n",
    "                f\"loss {cur_loss:5.2f} | mse {cur_mse:5.2f} |\"\n",
    "            )\n",
    "            total_loss = 0\n",
    "            total_mse = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "def eval_perturb(\n",
    "    loader: DataLoader, model: TransformerGenerator, device: torch.device\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Run model in inference mode using a given data loader\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    pert_cat = []\n",
    "    pred = []\n",
    "    truth = []\n",
    "    pred_de = []\n",
    "    truth_de = []\n",
    "    results = {}\n",
    "    logvar = []\n",
    "\n",
    "    for itr, batch in enumerate(loader):\n",
    "        batch.to(device)\n",
    "        pert_cat.extend(batch.pert)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            p = model.pred_perturb(\n",
    "                batch,\n",
    "                include_zero_gene=include_zero_gene,\n",
    "                gene_ids=gene_ids,\n",
    "            )\n",
    "            t = batch.y\n",
    "            pred.extend(p.cpu())\n",
    "            truth.extend(t.cpu())\n",
    "\n",
    "            # Differentially expressed genes\n",
    "            for itr, de_idx in enumerate(batch.de_idx):\n",
    "                pred_de.append(p[itr, de_idx])\n",
    "                truth_de.append(t[itr, de_idx])\n",
    "\n",
    "    # all genes\n",
    "    results[\"pert_cat\"] = np.array(pert_cat)\n",
    "    pred = torch.stack(pred)\n",
    "    truth = torch.stack(truth)\n",
    "    results[\"pred\"] = pred.detach().cpu().numpy().astype(np.float64)\n",
    "    results[\"truth\"] = truth.detach().cpu().numpy().astype(np.float64)\n",
    "\n",
    "    pred_de = torch.stack(pred_de)\n",
    "    truth_de = torch.stack(truth_de)\n",
    "    results[\"pred_de\"] = pred_de.detach().cpu().numpy().astype(np.float64)\n",
    "    results[\"truth_de\"] = truth_de.detach().cpu().numpy().astype(np.float64)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def enable_gradient_checkpointing(model):\n",
    "    \"\"\"Enable gradient checkpointing for a model.\"\"\"\n",
    "    if hasattr(model, \"gradient_checkpointing_enable\"):\n",
    "        model.gradient_checkpointing_enable()\n",
    "    else:\n",
    "        # Manual implementation for models without built-in support\n",
    "        for module in model.modules():\n",
    "            if hasattr(module, \"checkpoint\") and not module.checkpoint:\n",
    "                module.checkpoint = True\n",
    "\n",
    "\n",
    "def predict(\n",
    "    model: TransformerGenerator, pert_list: List[str], pool_size: Optional[int] = None\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Predict the gene expression values for the given perturbations.\n",
    "\n",
    "    Args:\n",
    "        model (:class:`torch.nn.Module`): The model to use for prediction.\n",
    "        pert_list (:obj:`List[str]`): The list of perturbations to predict.\n",
    "        pool_size (:obj:`int`, optional): For each perturbation, use this number\n",
    "            of cells in the control and predict their perturbation results. Report\n",
    "            the stats of these predictions. If `None`, use all control cells.\n",
    "    \"\"\"\n",
    "    adata = pert_data.adata\n",
    "    ctrl_adata = adata[adata.obs[\"condition\"] == \"ctrl\"]\n",
    "    if pool_size is None:\n",
    "        pool_size = len(ctrl_adata.obs)\n",
    "    gene_list = pert_data.gene_names.values.tolist()\n",
    "    for pert in pert_list:\n",
    "        for i in pert:\n",
    "            if i not in gene_list:\n",
    "                raise ValueError(\n",
    "                    \"The gene is not in the perturbation graph. Please select from GEARS.gene_list!\"\n",
    "                )\n",
    "\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    with torch.no_grad():\n",
    "        results_pred = {}\n",
    "        for pert in pert_list:\n",
    "            cell_graphs = create_cell_graph_dataset_for_prediction(\n",
    "                pert, ctrl_adata, gene_list, device, num_samples=pool_size\n",
    "            )\n",
    "            loader = DataLoader(cell_graphs, batch_size=eval_batch_size, shuffle=False)\n",
    "            preds = []\n",
    "            for batch_data in loader:\n",
    "                pred_gene_values = model.pred_perturb(\n",
    "                    batch_data, include_zero_gene, gene_ids=gene_ids, amp=amp\n",
    "                )\n",
    "                preds.append(pred_gene_values)\n",
    "            preds = torch.cat(preds, dim=0)\n",
    "            results_pred[\"_\".join(pert)] = np.mean(preds.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "    return results_pred\n",
    "\n",
    "\n",
    "def plot_perturbation(\n",
    "    model: nn.Module, query: str, save_file: str = None, pool_size: int = None\n",
    ") -> matplotlib.figure.Figure:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "\n",
    "    sns.set_theme(style=\"ticks\", rc={\"axes.facecolor\": (0, 0, 0, 0)}, font_scale=1.5)\n",
    "\n",
    "    adata = pert_data.adata\n",
    "    gene2idx = pert_data.node_map\n",
    "    cond2name = dict(adata.obs[[\"condition\", \"condition_name\"]].values)\n",
    "    gene_raw2id = dict(zip(adata.var.index.values, adata.var.gene_name.values))\n",
    "\n",
    "    de_idx = [\n",
    "        gene2idx[gene_raw2id[i]]\n",
    "        for i in adata.uns[\"top_non_dropout_de_20\"][cond2name[query]]\n",
    "    ]\n",
    "    genes = [\n",
    "        gene_raw2id[i] for i in adata.uns[\"top_non_dropout_de_20\"][cond2name[query]]\n",
    "    ]\n",
    "    truth = adata[adata.obs.condition == query].X.toarray()[:, de_idx]\n",
    "    if query.split(\"+\")[1] == \"ctrl\":\n",
    "        pred = predict(model, [[query.split(\"+\")[0]]], pool_size=pool_size)\n",
    "        pred = pred[query.split(\"+\")[0]][de_idx]\n",
    "    else:\n",
    "        pred = predict(model, [query.split(\"+\")], pool_size=pool_size)\n",
    "        pred = pred[\"_\".join(query.split(\"+\"))][de_idx]\n",
    "    ctrl_means = adata[adata.obs[\"condition\"] == \"ctrl\"].to_df().mean()[de_idx].values\n",
    "\n",
    "    pred = pred - ctrl_means\n",
    "    truth = truth - ctrl_means\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=[16.5, 4.5])\n",
    "    plt.title(query)\n",
    "    plt.boxplot(truth, showfliers=False, medianprops=dict(linewidth=0))\n",
    "\n",
    "    for i in range(pred.shape[0]):\n",
    "        _ = plt.scatter(i + 1, pred[i], color=\"red\")\n",
    "\n",
    "    plt.axhline(0, linestyle=\"dashed\", color=\"green\")\n",
    "\n",
    "    ax.xaxis.set_ticklabels(genes, rotation=90)\n",
    "\n",
    "    plt.ylabel(\"Change in Gene Expression over Control\", labelpad=10)\n",
    "    plt.tick_params(axis=\"x\", which=\"major\", pad=5)\n",
    "    plt.tick_params(axis=\"y\", which=\"major\", pad=5)\n",
    "    sns.despine()\n",
    "\n",
    "    if save_file:\n",
    "        fig.savefig(save_file, bbox_inches=\"tight\", transparent=False)\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "# CONFIG\n",
    "# ---------------------------------------------------------------------------------------\n",
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "# torch.cuda.set_per_process_memory_fraction(0.95)  # Use 95% of available memory\n",
    "matplotlib.rcParams[\"savefig.transparent\"] = False\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# settings for data prcocessing\n",
    "pad_token = \"<pad>\"\n",
    "special_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\n",
    "pad_value = 0  # for padding values\n",
    "pert_pad_id = 0\n",
    "include_zero_gene = \"all\"\n",
    "max_seq_len = 1536\n",
    "\n",
    "# settings for training\n",
    "MLM = True  # whether to use masked language modeling, currently it is always on.\n",
    "CLS = False  # celltype classification objective\n",
    "CCE = False  # Contrastive cell embedding objective\n",
    "MVC = False  # Masked value prediction for cell embedding\n",
    "ECS = False  # Elastic cell similarity objective\n",
    "amp = True\n",
    "\n",
    "# settings for optimizer\n",
    "lr = 1e-4  # or 1e-4\n",
    "batch_size = 32\n",
    "eval_batch_size = 32\n",
    "accumulation_steps = 2\n",
    "epochs = 15\n",
    "schedule_interval = 1\n",
    "early_stop = 10\n",
    "\n",
    "# settings for the model\n",
    "default_model_settings = {\n",
    "    \"embsize\": 512,  # embedding dimension\n",
    "    \"d_hid\": 512,  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "    \"nlayers\": 12,  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "    \"nhead\": 8,  # number of heads in nn.MultiheadAttention\n",
    "    \"n_layers_cls\": 3,\n",
    "    \"dropout\": 0,  # dropout probability\n",
    "    \"use_fast_transformer\": True,  # whether to use fast transformer\n",
    "}\n",
    "\n",
    "# logging\n",
    "log_interval = 100\n",
    "\n",
    "\n",
    "# dataset and evaluation choices\n",
    "data_name = \"adamson\"\n",
    "split = \"simulation\"\n",
    "if data_name == \"norman\":\n",
    "    perts_to_plot = [\"SAMD1+ZBTB1\"]\n",
    "elif data_name == \"adamson\":\n",
    "    perts_to_plot = [\"KCTD16+ctrl\"]\n",
    "\n",
    "\n",
    "# SETUP\n",
    "# ---------------------------------------------------------------------------------------\n",
    "# parser = argparse.ArgumentParser(description=\"scGPT Perturb\")\n",
    "# parser.add_argument(\n",
    "#     \"-d\",\n",
    "#     \"--data_dir\",\n",
    "#     type=str,\n",
    "#     required=False,\n",
    "#     help=\"Directory containing the data\",\n",
    "# )\n",
    "# parser.add_argument(\n",
    "#     \"-o\",\n",
    "#     \"--output_dir\",\n",
    "#     type=str,\n",
    "#     required=False,\n",
    "#     help=\"Directory to save the output\",\n",
    "# )\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# data_dir = args.data_dir\n",
    "# output_dir = args.output_dir\n",
    "data_dir = \"data\"\n",
    "output_dir = \"output\"\n",
    "save_dir = os.path.join(\n",
    "    data_dir, \"save\", f\"dev_perturb_{data_name}-{time.strftime('%b%d-%H-%M')}\"\n",
    ")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(os.path.join(data_dir, \"scGPT_human\")):\n",
    "    subprocess.run(\n",
    "        [\n",
    "            \"gdown\",\n",
    "            \"--folder\",\n",
    "            \"https://drive.google.com/drive/folders/1oWh_-ZRdhtoGQ2Fw24HP41FgLoomVo-y\",\n",
    "            \"-O\",\n",
    "            os.path.join(data_dir, \"scGPT_human\"),\n",
    "        ],\n",
    "        check=True,\n",
    "    )\n",
    "\n",
    "load_model = os.path.join(data_dir, \"scGPT_human\")\n",
    "load_param_prefixs = [\n",
    "    \"encoder\",\n",
    "    \"value_encoder\",\n",
    "    \"transformer_encoder\",\n",
    "]\n",
    "\n",
    "\n",
    "logger = scg.logger\n",
    "scg.utils.add_file_handler(logger, os.path.join(save_dir, \"run.log\"))\n",
    "logger.info(f\"Running on {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "\n",
    "# RUN\n",
    "# ---------------------------------------------------------------------------------------\n",
    "pert_data, vocab, gene_ids, n_genes = prepare_data(\n",
    "    batch_size, eval_batch_size, data_dir, data_name, split\n",
    ")\n",
    "\n",
    "# model = setup_model(\n",
    "#     load_model=load_model,\n",
    "#     vocab=vocab,\n",
    "#     genes=pert_data.adata.var[\"gene_name\"].tolist(),\n",
    "#     default_config=default_model_settings,\n",
    "# )\n",
    "\n",
    "# # criterion_cls = nn.CrossEntropyLoss()\n",
    "# criterion = masked_mse_loss\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, schedule_interval, gamma=0.9)\n",
    "\n",
    "# # TODO return from metal\n",
    "# # scaler = torch.cuda.amp.GradScaler(enabled=amp)\n",
    "# scaler = torch.amp.GradScaler(enabled=amp)\n",
    "\n",
    "# for line in torch.cuda.memory_summary().split(\"\\n\"):\n",
    "#     print(line)\n",
    "\n",
    "# best_val_loss = float(\"inf\")\n",
    "# best_val_corr = 0\n",
    "# sest_model = None\n",
    "# patience = 0\n",
    "\n",
    "# for epoch in range(1, epochs + 1):\n",
    "#     epoch_start_time = time.time()\n",
    "#     train_loader = pert_data.dataloader[\"train_loader\"]\n",
    "#     valid_loader = pert_data.dataloader[\"val_loader\"]\n",
    "\n",
    "#     train(\n",
    "#         model=model,\n",
    "#         train_loader=train_loader,\n",
    "#         n_genes=n_genes,\n",
    "#         criterion=criterion,\n",
    "#         optimizer=optimizer,\n",
    "#         scheduler=scheduler,\n",
    "#         scaler=scaler,\n",
    "#     )\n",
    "\n",
    "#     val_res = eval_perturb(valid_loader, model, device)\n",
    "#     val_metrics = compute_perturbation_metrics(\n",
    "#         val_res, pert_data.adata[pert_data.adata.obs[\"condition\"] == \"ctrl\"]\n",
    "#     )\n",
    "\n",
    "#     logger.info(f\"val_metrics at epoch {epoch}: \")\n",
    "#     logger.info(val_metrics)\n",
    "\n",
    "#     elapsed = time.time() - epoch_start_time\n",
    "#     logger.info(f\"| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | \")\n",
    "\n",
    "#     val_score = val_metrics[\"pearson\"]\n",
    "#     if val_score > best_val_corr:\n",
    "#         best_val_corr = val_score\n",
    "#         best_model = copy.deepcopy(model)\n",
    "#         logger.info(f\"Best model with score {val_score:5.4f}\")\n",
    "#         patience = 0\n",
    "#     else:\n",
    "#         patience += 1\n",
    "#         if patience >= early_stop:\n",
    "#             logger.info(f\"Early stop at epoch {epoch}\")\n",
    "#             break\n",
    "\n",
    "#     torch.save(\n",
    "#         model.state_dict(),\n",
    "#         os.path.join(save_dir, f\"model_{epoch}.pt\"),\n",
    "#     )\n",
    "\n",
    "#     scheduler.step()\n",
    "\n",
    "# torch.save(best_model.state_dict(), os.path.join(save_dir, \"best_model.pt\"))\n",
    "\n",
    "# \"\"\" ## Evaluations\"\"\"\n",
    "\n",
    "# for p in perts_to_plot:\n",
    "#     plot_perturbation(\n",
    "#         best_model, p, pool_size=300, save_file=os.path.join(output_dir, f\"{p}.png\")\n",
    "#     )\n",
    "\n",
    "# test_loader = pert_data.dataloader[\"test_loader\"]\n",
    "# test_res = eval_perturb(test_loader, best_model, device)\n",
    "\n",
    "# # test_metrics, test_pert_res = compute_metrics(test_res)\n",
    "# test_metrics = compute_perturbation_metrics(\n",
    "#     test_res, pert_data.adata[pert_data.adata.obs[\"condition\"] == \"ctrl\"]\n",
    "# )\n",
    "# print(test_metrics)\n",
    "\n",
    "# # save the dicts in json\n",
    "# with open(os.path.join(output_dir, \"test_metrics.json\"), \"w\") as f:\n",
    "#     json.dump(test_metrics, f)\n",
    "# # with open(f\"{save_dir}/test_pert_res.json\", \"w\") as f:\n",
    "# #     json.dump(test_pert_res, f)\n",
    "\n",
    "# deeper_res = deeper_analysis(pert_data.adata, test_res)\n",
    "# non_dropout_res = non_dropout_analysis(pert_data.adata, test_res)\n",
    "\n",
    "# metrics = [\"pearson_delta\", \"pearson_delta_de\"]\n",
    "# metrics_non_dropout = [\n",
    "#     \"pearson_delta_top20_de_non_dropout\",\n",
    "#     \"pearson_top20_de_non_dropout\",\n",
    "# ]\n",
    "# subgroup_analysis = {}\n",
    "# for name in pert_data.subgroup[\"test_subgroup\"].keys():\n",
    "#     subgroup_analysis[name] = {}\n",
    "#     for m in metrics:\n",
    "#         subgroup_analysis[name][m] = []\n",
    "\n",
    "#     for m in metrics_non_dropout:\n",
    "#         subgroup_analysis[name][m] = []\n",
    "\n",
    "# for name, pert_list in pert_data.subgroup[\"test_subgroup\"].items():\n",
    "#     for pert in pert_list:\n",
    "#         for m in metrics:\n",
    "#             subgroup_analysis[name][m].append(deeper_res[pert][m])\n",
    "\n",
    "#         for m in metrics_non_dropout:\n",
    "#             subgroup_analysis[name][m].append(non_dropout_res[pert][m])\n",
    "\n",
    "# for name, result in subgroup_analysis.items():\n",
    "#     for m in result.keys():\n",
    "#         mean_value = np.mean(subgroup_analysis[name][m])\n",
    "#         logger.info(\"test_\" + name + \"_\" + m + \": \" + str(mean_value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = os.path.join(load_model)\n",
    "model_config_file = os.path.join(model_dir, \"args.json\")\n",
    "\n",
    "# vocab_file = os.path.join(model_dir, \"vocab.json\")\n",
    "# vocab = GeneVocab.from_file(vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - match 4399/5060 genes in vocabulary of size 60697.\n",
      "scGPT - INFO - Resume model from data/scGPT_human/best_model.pt, the model args will override the config data/scGPT_human/args.json.\n",
      "5063\n",
      "scGPT - INFO - Loading params encoder.embedding.weight with shape torch.Size([60697, 512])\n",
      "scGPT - INFO - Loading params encoder.enc_norm.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params encoder.enc_norm.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])\n",
      "scGPT - INFO - Loading params value_encoder.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params value_encoder.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.norm.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.norm.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.in_proj_weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.in_proj_bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.in_proj_weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.in_proj_bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.in_proj_weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.in_proj_bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.in_proj_weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.in_proj_bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.in_proj_weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.in_proj_bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.in_proj_weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.in_proj_bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.in_proj_weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.in_proj_bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.in_proj_weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.in_proj_bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.in_proj_weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.in_proj_bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.in_proj_weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.in_proj_bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.in_proj_weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.in_proj_bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.in_proj_weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.in_proj_bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for TransformerGenerator:\n\tsize mismatch for encoder.embedding.weight: copying a param with shape torch.Size([60697, 512]) from checkpoint, the shape in current model is torch.Size([5063, 512]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 93\u001b[0m\n\u001b[1;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading params \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     92\u001b[0m     model_dict\u001b[38;5;241m.\u001b[39mupdate(pretrained_dict)\n\u001b[0;32m---> 93\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m load_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/School/wei/pegasus/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:2189\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2184\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2185\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2186\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2190\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for TransformerGenerator:\n\tsize mismatch for encoder.embedding.weight: copying a param with shape torch.Size([60697, 512]) from checkpoint, the shape in current model is torch.Size([5063, 512])."
     ]
    }
   ],
   "source": [
    "default_config = default_model_settings\n",
    "genes = pert_data.adata.var[\"gene_name\"].tolist()\n",
    "\n",
    "# Unpack default model settings\n",
    "embsize = default_config[\"embsize\"]\n",
    "d_hid = default_config[\"d_hid\"]\n",
    "nlayers = default_config[\"nlayers\"]\n",
    "nhead = default_config[\"nhead\"]\n",
    "n_layers_cls = default_config[\"n_layers_cls\"]\n",
    "dropout = default_config[\"dropout\"]\n",
    "use_fast_transformer = default_config[\"use_fast_transformer\"]\n",
    "pretrained_dict = None\n",
    "\n",
    "# Update model settings from checkpoint, if provided\n",
    "if load_model is not None and os.path.exists(load_model):\n",
    "    model_dir = os.path.join(load_model)\n",
    "    model_config_file = os.path.join(model_dir, \"args.json\")\n",
    "    model_file = os.path.join(model_dir, \"best_model.pt\")\n",
    "    vocab_file = os.path.join(model_dir, \"vocab.json\")\n",
    "\n",
    "    # Update vocab if loading from existing model\n",
    "    loaded_vocab = GeneVocab.from_file(vocab_file)\n",
    "    for s in special_tokens:\n",
    "        if s not in loaded_vocab:\n",
    "            loaded_vocab.append_token(s)\n",
    "\n",
    "    # Check gene vocabulary matching\n",
    "    gene_ids_in_vocab = np.array([1 if gene in loaded_vocab else -1 for gene in genes])\n",
    "    logger.info(\n",
    "        f\"match {np.sum(gene_ids_in_vocab >= 0)}/{len(gene_ids_in_vocab)} genes \"\n",
    "        f\"in vocabulary of size {len(loaded_vocab)}.\"\n",
    "    )\n",
    "\n",
    "    # Load model configurations\n",
    "    with open(model_config_file, \"r\") as f:\n",
    "        model_configs = json.load(f)\n",
    "\n",
    "    logger.info(\n",
    "        f\"Resume model from {model_file}, the model args will override the \"\n",
    "        f\"config {model_config_file}.\"\n",
    "    )\n",
    "\n",
    "    # Extract model parameters\n",
    "    embsize = model_configs[\"embsize\"]\n",
    "    nhead = model_configs[\"nheads\"]\n",
    "    d_hid = model_configs[\"d_hid\"]\n",
    "    nlayers = model_configs[\"nlayers\"]\n",
    "    n_layers_cls = model_configs[\"n_layers_cls\"]\n",
    "\n",
    "    # Load pretrained weights\n",
    "    pretrained_dict = torch.load(model_file, map_location=torch.device(device))\n",
    "    pretrained_dict = convert_wqkv_to_in_proj(pretrained_dict)\n",
    "else:\n",
    "    genes = pert_data.adata.var[\"gene_name\"].tolist()\n",
    "    vocab = Vocab(\n",
    "        VocabPybind(genes + special_tokens, None)\n",
    "    )  # bidirectional lookup [gene <-> int]\n",
    "\n",
    "ntokens = len(vocab)  # size of vocabulary\n",
    "print(ntokens)\n",
    "model = TransformerGenerator(\n",
    "    ntokens,\n",
    "    embsize,\n",
    "    nhead,\n",
    "    d_hid,\n",
    "    nlayers,\n",
    "    nlayers_cls=n_layers_cls,\n",
    "    n_cls=1,\n",
    "    vocab=vocab,\n",
    "    dropout=dropout,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    pert_pad_id=pert_pad_id,\n",
    "    use_fast_transformer=use_fast_transformer,\n",
    ")\n",
    "\n",
    "# Load pretrained weights\n",
    "if (\n",
    "    load_param_prefixs is not None\n",
    "    and load_model is not None\n",
    "    and pretrained_dict is not None\n",
    "):\n",
    "    # only load params that start with the prefix\n",
    "    model_dict = model.state_dict()\n",
    "    pretrained_dict = {\n",
    "        k: v\n",
    "        for k, v in pretrained_dict.items()\n",
    "        if any([k.startswith(prefix) for prefix in load_param_prefixs])\n",
    "    }\n",
    "    for k, v in pretrained_dict.items():\n",
    "        logger.info(f\"Loading params {k} with shape {v.shape}\")\n",
    "    model_dict.update(pretrained_dict)\n",
    "    model.load_state_dict(model_dict, strict=False)\n",
    "elif load_model is not None:\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_file))\n",
    "        logger.info(f\"Loading all model params from {model_file}\")\n",
    "    except RuntimeError:\n",
    "        # only load params that are in the model and match the size\n",
    "        model_dict = model.state_dict()\n",
    "        pretrained_dict = torch.load(model_file)\n",
    "        pretrained_dict = {\n",
    "            k: v\n",
    "            for k, v in pretrained_dict.items()\n",
    "            if k in model_dict and v.shape == model_dict[k].shape\n",
    "        }\n",
    "        for k, v in pretrained_dict.items():\n",
    "            logger.info(f\"Loading params {k} with shape {v.shape}\")\n",
    "        model_dict.update(pretrained_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "\n",
    "model.to(device)\n",
    "enable_gradient_checkpointing(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerGenerator(\n",
       "  (encoder): GeneEncoder(\n",
       "    (embedding): Embedding(60697, 512, padding_idx=60694)\n",
       "    (enc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (value_encoder): ContinuousValueEncoder(\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "    (linear1): Linear(in_features=1, out_features=512, bias=True)\n",
       "    (activation): ReLU()\n",
       "    (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (pert_encoder): Embedding(3, 512, padding_idx=0)\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0, inplace=False)\n",
       "        (dropout2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): AffineExprDecoder(\n",
       "    (coeff_decoder): ExprDecoder(\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (3): LeakyReLU(negative_slope=0.01)\n",
       "        (4): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (bias_decoder): ExprDecoder(\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (3): LeakyReLU(negative_slope=0.01)\n",
       "        (4): Linear(in_features=512, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls_decoder): ClsDecoder(\n",
       "    (_decoder): ModuleList(\n",
       "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (4): ReLU()\n",
       "      (5): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (out_layer): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
